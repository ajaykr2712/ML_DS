model:
  name: "gpt2"
  architecture: "transformer"
  vocab_size: 50257
  d_model: 768
  num_layers: 12
  num_heads: 12
  d_ff: 3072
  max_seq_length: 1024
  dropout: 0.1
  layer_norm_eps: 1e-5
  
training:
  batch_size: 8
  gradient_accumulation_steps: 8
  learning_rate: 5e-5
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.999
  eps: 1e-8
  max_epochs: 10
  warmup_steps: 500
  max_grad_norm: 1.0
  
  # Learning rate scheduler
  scheduler:
    type: "cosine"
    T_max: 10000
    eta_min: 1e-6
  
  # Mixed precision training
  mixed_precision: true
  fp16: true
  
data:
  train_dataset: "data/train.jsonl"
  val_dataset: "data/val.jsonl"
  test_dataset: "data/test.jsonl"
  tokenizer: "gpt2"
  max_length: 1024
  num_workers: 4
  
generation:
  max_new_tokens: 100
  temperature: 0.8
  top_k: 50
  top_p: 0.9
  repetition_penalty: 1.1
  do_sample: true
  
logging:
  log_level: "INFO"
  log_dir: "logs"
  wandb:
    project: "gen-ai-gpt"
    entity: "your-team"
    tags: ["gpt", "language-model"]
  
checkpointing:
  save_dir: "checkpoints"
  save_every_n_epochs: 1
  keep_last_n: 3
  
evaluation:
  eval_every_n_steps: 1000
  metrics: ["perplexity", "bleu"]
  
distributed:
  backend: "nccl"
  world_size: 1
  rank: 0
